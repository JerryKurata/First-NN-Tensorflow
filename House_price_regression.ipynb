{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-Getting-Started -M4-TF-House-price-regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JerryKurata/First-NN-Tensorflow/blob/master/House_price_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpzj90oD__lN",
        "colab_type": "text"
      },
      "source": [
        "# Tensor Flow House Price Prediction\n",
        "\n",
        "This is a simple Tensorflow based notebook that applies the steps of the machine learning workflow to create, train, and test a model that will predict house prices.\n",
        "\n",
        "The notebook:\n",
        "\n",
        "*  Uses the steps of the machine workflow discussed in the course\n",
        "*  Illustrates how TensorFlow makes it easier to implement a machine learning solution\n",
        "*  Describes what is happening in various processing steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMyAUbOqOGgU",
        "colab_type": "text"
      },
      "source": [
        "## Load correct version of TensorFlow\n",
        "Before we use TensorFlow we must load the correct version.  We want version 2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwTnmk_IPQbz",
        "colab_type": "code",
        "outputId": "84ad0d4e-d0d7-4fce-fbcd-3e32e1d40251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "source": [
        "!pip install tensorflow==2.0\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.17.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaSlAAqoIiv6",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "We import TensorFlow, Numpy, and matplotlib libraries.  \n",
        "\n",
        "Numpy is a powerful N-dimensional array library that\n",
        "allows us to easily create and manipulate arrays of data, and more!\n",
        "\n",
        "Numpy also allows us to convert TensorFlow's native data structures,\n",
        "to python native data types.\n",
        "\n",
        "matplotlib is a graphics plot library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVJKEAqsIXEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vjZGJOwSxmR",
        "colab_type": "code",
        "outputId": "85217558-978c-49bd-8671-d1b6b5a1c874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# In colab we can turn on GPU acceleration in Notebook settings\n",
        "print(\"GPU is available:\", tf.test.is_gpu_available())\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.0.0\n",
            "GPU is available: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9071f7abaa3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# In colab we can turn on GPU acceleration in Notebook settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU is available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.config' has no attribute 'list_physical_devices'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHIiwqsORnEb",
        "colab_type": "text"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "Using example data, develop a model that predicts house prices based on the size of a house.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7iNMKHlJa-F",
        "colab_type": "text"
      },
      "source": [
        "## Get Data\n",
        "\n",
        "In this example we will be using a truncated version of the Ames dataset that only contains information on homes sold in May 2010.\n",
        "\n",
        "### About the Ames dataset.\n",
        "\n",
        "The Ames dataset is a widely avalable dataset that has become one of the standard datasets used when\n",
        "predicting home prices based on features of the home. It is based on the great work of Dean De Cock.  His rational\n",
        "and insight into this dataset can be found at http://jse.amstat.org/v19n3/decock.pdf.\n",
        "\n",
        "\n",
        "### Getting the truncated dataset we use\n",
        "This dataset can be found with the exercise files for this course.  The filename is **AmesHousing-05-2010.csv** .\n",
        "\n",
        "Before you run the code below, ensure you have downloaded the file to your computer.  \n",
        "\n",
        "And when prompted, browse to the file's location on your computer and upload the file.\n",
        "\n",
        "*Warning: The file upload function will only show the file selection dialog for 30 seconds.  After that time it will close and cause an error because a file was not specified.  This timeout prevents the file dialog from blocking events in colab.  If you experience this timeout select the dataset file sooner.  It may help to move the dataset file to an easier to specify location such as your desktop.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtsmZTmC6qcS",
        "colab_type": "code",
        "outputId": "7d948ca4-7eb7-4dee-d196-423b48955f94",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "# You need to upload the file, AmesHousing-05-2001.csv provided with this course\n",
        "from google.colab import files\n",
        "\n",
        "# only load the single file\n",
        "uploaded = files.upload()\n",
        "# csv_housefile contains the name of the first, and only file uploaded\n",
        "csv_housefile = next(iter(uploaded.keys()))\n",
        "\n",
        "print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=csv_housefile, length=len(uploaded[csv_housefile])))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff7d9cad-7dcc-463f-be45-529651ca6d5a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ff7d9cad-7dcc-463f-be45-529651ca6d5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-868159241d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# csv_housefile contains the name of the first, and only file uploaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcsv_housefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hiXAp_v6urP",
        "colab_type": "text"
      },
      "source": [
        "### Load the data into a pandas dataframe\n",
        "\n",
        "Pandas let's us easily review and manipulate the data.  If you are new to pandas, see https://pandas.pydata.org/ and the pandas 10 minute intro  http://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html to understand how pandas, and pandas dataframes makes working with tabular data in Python easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zveOHSXBItTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the housing data into a pandas dataframe for easy viewing and manipulation\n",
        "df_housing = pd.read_csv(csv_housefile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC9Ygn0jynyW",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the House Price data\n",
        "\n",
        "We use panda's  head method to look at the first few rows of data.  This will give us an understanding of the data we are working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtdGnno4zBeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the dataset\n",
        " \n",
        "# turn on option to display all columns, otherwise some columns may be hidden\n",
        "pd.set_option('display.max_columns', None)  \n",
        "# print the column names and first 5 rows of data\n",
        "df_housing.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8urfea9zAhH",
        "colab_type": "text"
      },
      "source": [
        "Lots of feature columns.  From documentation of previous work with this data we can account for most of the difference in price (SalePrice) based on the size of the house. \n",
        "\n",
        "Looking through the columns we see the following features contains square feet ('SF'): BsmtFin SF 1, BsmtFin SF 2, Bsmt Unf SF, Total Bsmt SF,  1st Flr SF, 2nd Flr SF, Low Qual Fin SF. \n",
        "\n",
        "A little more checking of the data and we see in the data that there are summary columns for the basement square footage and the above basement square footage.  Specifically the feature columns Total Bsmt SF and Gr Liv Area sum the space as follows:\n",
        "\n",
        "\n",
        "\n",
        ">  Total Bsmt SF = BsmtFin SF 1 +  BsmtFin SF 2 + Bsmt Unf SF \n",
        "\n",
        "> Gr Liv Area =  1st Flr SF +  2nd Flr SF\n",
        "\n",
        "But, there is no feature for the total space in the house which is the sum of Basement and Upper floors or (Total Bsmt SF + Gr Liv Area ).  So let's add a new feature column containing this value called 'Total SF' to the dataset.\n",
        "\n",
        "But, before we proceed, let's see if the feature columns we are dependent upon contain missing values that will effect our results.  If so, we can remove the rows with missing columns or determine a way of imputing (updating the values) the missing columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gc0jzZYvLsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " df_housing[['Total Bsmt SF', 'Gr Liv Area']].isnull().values.any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDHSGCWCw6KJ",
        "colab_type": "text"
      },
      "source": [
        "No missing values, so no additional work is required.  So we can add the new column Total SF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh3dHm0U-aY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add new feature column Total SF = Total Bsmt SF + Gr Liv Area\n",
        "df_housing['Total SF'] = df_housing['Total Bsmt SF'] + df_housing['Gr Liv Area']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8WXLZlB3l0N",
        "colab_type": "text"
      },
      "source": [
        "And we can check that the numbers add and see associated sale price "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4LQHNxWDBRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see the  bsmt and living area, total sf, and price columns\n",
        "print(df_housing[['Total Bsmt SF', 'Gr Liv Area', 'Total SF', 'SalePrice']].head(5) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aORObOZrKIMA",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the data.  \n",
        "\n",
        "We visualize the data to give us some idea of the relationships between the various features.  To do that we create a  function to plot the data.\n",
        " \n",
        " \n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7hTDDs6qw0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function visualizes our data and optionally a learned line\n",
        "def visualize_data(x_vals, y_vals,\n",
        "                   addn_x_vals=None, addn_y_vals=None, add_addn_reg_line=False):\n",
        "  \n",
        "  f, ax = plt.subplots(figsize=(8,8))\n",
        "  plt.plot(x_vals, y_vals, 'ro')   # red dot for each data point\n",
        "  # Optional plot another set of data points in a different color and symbol\n",
        "  if (addn_x_vals is not None):\n",
        "    plt.plot(addn_x_vals, addn_y_vals, 'g^') # green triangles for addition data points\n",
        "    # Optionally, plot a regression line.\n",
        "    if (add_addn_reg_line):\n",
        "      x_min_index = addn_x_vals.argmin()\n",
        "      x_max_index = addn_x_vals.argmax()\n",
        "      print(x_min_index,[addn_x_vals[x_min_index],addn_y_vals[x_min_index]] ) \n",
        "      print(x_max_index,[addn_x_vals[x_max_index],addn_y_vals[x_max_index]] ) \n",
        "      plt.plot([addn_x_vals[x_min_index],addn_y_vals[x_min_index]], \n",
        "               [addn_x_vals[x_max_index],addn_y_vals[x_max_index]], \n",
        "               'b-')  # draw a blue regression line\n",
        "    \n",
        "  plt.tick_params(axis='both', which='major', labelsize=14)\n",
        "  \n",
        "  plt.show()  # now plot the line showing the data and optionally the line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d887fwIbFP1",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Total SF and Price\n",
        "\n",
        "Using the visualize data function we can see the relationship between Total Square Feet (Total SF) and Price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PyYXnSZ9v9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Total SF vs. Price\n",
        "visualize_data(df_housing['Total SF'], df_housing['SalePrice'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJjuAi3sb76r",
        "colab_type": "text"
      },
      "source": [
        "### What does the visualization tell us?\n",
        "\n",
        "There seems to be a **linear** relationship between the size of the house (as shown in Total SF) and Price.  \n",
        "\n",
        "This suggests that we should perform Linear Regression. Where the relationship in the data is:\n",
        " y = mx + b.  \n",
        " Or in our case, Price = m * Total SF + b\n",
        " \n",
        "With respect to defining the appropriate equation, we can see the difference between traditional programming and Machine Learning as:\n",
        "\n",
        "* In tradition programming **we define what the equation** is through structures such as conditional statements\n",
        "\n",
        "* In Machine Learning, our model **learns from the data** what the appropriate equation is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWm_EBhwfTvX",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "If values are on very different scales it will be difficult for the model to determine the relationships between features.  With our data, Square Footage(SF) ranges from 800-4200, and Prices range from 80,000 to 400,0000.  This means there is a nearly 100 times difference in scale.  When we normalize we will reduce both qualtities to the same scale while preserving the differences between prices and sizes of homes.  This will help our model learn the relationship between price and size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-zGuMN6np6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale data so SF and Sale Price are on similar scales with values \n",
        "#  from 0.0 to 1.0\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sf_scaler = MinMaxScaler()\n",
        "sf_scaled = sf_scaler.fit_transform(df_housing['Total SF'].values.reshape(-1,1).astype(np.float64))\n",
        "    \n",
        "price_scaler = MinMaxScaler()\n",
        "price_scaled = price_scaler.fit_transform(df_housing['SalePrice'].values.reshape(-1,1).astype(np.float64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSO8dVIW5Fb",
        "colab_type": "text"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "Here we create a model to learn from our data.  Our data looks linearly related, so we can use a straight line\n",
        "to fit our data.  So we are going to create a model that is based on the equation of a straight line.\n",
        "\n",
        "From our training data we will learn that once we can define this line's parameters we can use it's equation to predict the value of any future data.\n",
        "\n",
        "We use Tensorflow's Keras library to make the process easy.\n",
        "\n",
        "We create a sequential model where the output of one layer becomes the input of the next layer.\n",
        "\n",
        "This model is a simple Neural Network that implements y = wx + bias, where **w is a weight** and **bias is an offset**.\n",
        "\n",
        "*You may recognize from Algebra or Geometry that y = wx + bias is the equation of straight line, where w is the slope and bias is the offset.  (See https://en.wikipedia.org/wiki/Line_(geometry) for a description.)*\n",
        "\n",
        "Therefore the goal is to use our data to train the model and learn the best values for w and bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o30MyqTYepiV",
        "colab_type": "text"
      },
      "source": [
        "### Our Model\n",
        "\n",
        "We create our linear model using TensorFlow's Keras library.   Using Keras makes it easy for us to create, train, and evaluate our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgcsUBmf_guO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model using the TensorFlow Keras library\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='linear', input_shape=[1],\n",
        "                                kernel_initializer='random_uniform',\n",
        "                                bias_initializer='zeros'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH040S8DdMTS",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "The model is now defined, but is not trained, or even ready to be trained.\n",
        "\n",
        "We will train the model by passing training data throught it and adjusting the weight and bias to reduce loss (error).  To perform these tasks we need a method for \n",
        "*  measuring loss and \n",
        "*  optimizing the values of the weight and bias to minimize this loss. \n",
        "\n",
        "There are many ways of measuring loss and optimizing the values.  We are going to use **Mean-Squared Error** ('mean_squared_error') to measure loss, and the process of **Stochasitic Gradient Descent** ('sgd') to find the optimal weight and bias to minimize the loss. *Note: We use the variant of sgd called mini-batch gradient descent.  And set the batch size in the fit method below.*\n",
        "\n",
        "We set these parameters for the model with the **compile** statement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMYudNJpY9dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model\n",
        "optimizer = \"sgd\"\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8a2y8xinVmf",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Once the model has been defined and compiled we can train the model.  We do this with training data.  \n",
        "\n",
        "We want to split our prepared dataset into 2 datasets.  One dataset will be used for training, and other dataset will used for testing.  **We never used testing data for training or training data for testing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeE7HxU_l0MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into training dataset - 70%, Testing dataset - 30%\n",
        "# we do this using the sklearn train_test_split method\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sf_train_scaled, sf_test_scaled, price_train_scaled, price_test_scaled = train_test_split(sf_scaled, \n",
        "                                                    price_scaled, \n",
        "                                                    test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJPTS2h1yGD3",
        "colab_type": "text"
      },
      "source": [
        "We pass the training data through the model multiple times.  On each pass, the loss function (that we defined in the model compile) will be used to calculate loss.  And the optimizer will be used to make adjustments to the weights and bias (just called weights from here on) to minimize this error.  \n",
        "\n",
        "We repeat this process of calculating the loss with the current weights and updating the weights to minimize loss for the specified number of **epochs**.  *We should see the loss decrease over time.*\n",
        "\n",
        "We can set the *verbose* flag to tell Keras how much information to show during the training process, 0 is none, 1 and 2 show more in-progress information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOCRKtgMnSLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Train model using data\n",
        "initial_epochs = 8\n",
        "batch_size = 10\n",
        "train_hist = model.fit(sf_train_scaled, price_train_scaled, \n",
        "                       epochs=initial_epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Is 8 epochs enough??? Maybe/Maybe not"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXB_5zlkvEpt",
        "colab_type": "text"
      },
      "source": [
        "### Visually Confirm Model Training\n",
        "\n",
        "Our model learned the best weight and bias for our training data that it could through the defined number of epochs.  Let's see if a line drawn with these learned weights and bias looks reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KPJ8zLCJYw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#  predict the price with our trained model\n",
        "price_predicted_scaled =  model.predict(sf_train_scaled)\n",
        "visualize_data(sf_scaler.inverse_transform(sf_train_scaled), price_scaler.inverse_transform(price_train_scaled),\n",
        "               sf_scaler.inverse_transform(sf_train_scaled), price_scaler.inverse_transform(price_predicted_scaled),\n",
        "               add_addn_reg_line=False)\n",
        "               \n",
        "# If line looks bad, what do we do?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP7E7zNb71zw",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Training\n",
        "\n",
        "\n",
        "Our trained model does not looks too good !!!\n",
        "\n",
        "What can we do?  \n",
        "\n",
        "Should we change model architecture?  Does linear look incorrect?  \n",
        "\n",
        "> NO\n",
        "\n",
        "\n",
        "What does loss look like?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVoztVaenFsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(hist):\n",
        "  # plot the loss\n",
        "  plt.title('Loss History')\n",
        "  plt.plot(hist.history['loss'])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()\n",
        "\n",
        "print(type(train_hist))\n",
        "plot_loss(train_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWEO8ONjm9gm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> LOSS STILL LOOKS HIGH.  BUT IS DECREASING.\n",
        "\n",
        "\n",
        "\n",
        "Perhaps we have not converged to the lowest loss.  What can we do????  \n",
        "\n",
        "Train with more data?  \n",
        "\n",
        "> NO.  WE HAVE ALL THE DATA\n",
        "\n",
        "Train for more epochs and check loss to ensure values have converged?\n",
        " > YES!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nez9pMAyhNll",
        "colab_type": "text"
      },
      "source": [
        "### Training more epochs\n",
        "\n",
        "The model is partially trained but the results are not great.  Let's go back and train the model some more. \n",
        "\n",
        "The question is how much more?  \n",
        "\n",
        "We want to ensure the model is converged to a range of loss values.   So what we will see in the training output is the loss value starts to stay in a specific range .   It may go up or down a little, but there should be a stable range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2azhJj0hd7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model using data.  This time, train for additional epochs\n",
        "# Also capture the changes over time in the history variable, train_hist so we \n",
        "#   can see if loss is converging\n",
        "addn_epochs = 1200\n",
        "train_hist_addn =model.fit(sf_train_scaled, price_train_scaled,  \n",
        "                     epochs=addn_epochs, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRhVDSCDj_pu",
        "colab_type": "text"
      },
      "source": [
        "Plot the loss history values to see if our model converged.\n",
        "\n",
        "Because a model can have a history of many  different metrics over time, there is a history object which has a keys dictionary of \n",
        "metrics whose values were stored on each epoch. By plotting the values for the 'loss' key we can answer:\n",
        "\n",
        "\n",
        "1.   Did the loss converge?\n",
        "2.   How many epochs were required for the loss to converge?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEXR-tQpjmJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_hist.history['loss'].extend(train_hist_addn.history['loss'])   # add the addition epochs to the  training history\n",
        "plot_loss(train_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTv9B0Inx19",
        "colab_type": "text"
      },
      "source": [
        "### Visually Confirm Model after Additional Training\n",
        "\n",
        "As the  plot showed, the model's loss converged by 1000 epochs.  So the model should be trained to allow us to make accurate predictions.  \n",
        "\n",
        "Let's again plot the predicted price for each size.  And see if a line drawn with the learned weight and bias looks reasonable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUntqHjoko-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize data and regression line for learned weights\n",
        "price_predicted_scaled =  model.predict(sf_train_scaled)\n",
        "visualize_data(sf_scaler.inverse_transform(sf_train_scaled), \n",
        "               price_scaler.inverse_transform(price_train_scaled),\n",
        "               sf_scaler.inverse_transform(sf_train_scaled), \n",
        "               price_scaler.inverse_transform(price_predicted_scaled),\n",
        "               add_addn_reg_line=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrrEbGQOvUrv",
        "colab_type": "text"
      },
      "source": [
        "**Yes.  The predicted line looks good!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZmXIWpZXmwe",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Trained Model\n",
        "\n",
        "Once we have the model trained, we want to test **with data not used to train the model!!!**\n",
        "\n",
        "The use of data not used in training to test the model is key.  If you use training data our testing results will be overly accurate and will not relect *\"real-life\"* usage of the trained model to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjV3pUQ4bP2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a price prediction on data the model has never seen before, i.e. \n",
        "#    the Test data square footage\n",
        "price_test_pred_scaled = model.predict(sf_test_scaled)\n",
        "\n",
        "# calculate the mean squared error for the prediction, lower is better\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"prediction mean squared error:\", \n",
        "          mean_squared_error(price_test_scaled, price_test_pred_scaled ))\n",
        "\n",
        "# Visualize data\n",
        "#    plus the test square footage and predicted price\n",
        "print('\\nGreen triangles are Test Square footage with predicted Price')\n",
        "visualize_data(sf_scaler.inverse_transform(sf_test_scaled),\n",
        "               price_scaler.inverse_transform(price_test_scaled),\n",
        "               sf_scaler.inverse_transform(sf_test_scaled), \n",
        "               price_scaler.inverse_transform(price_test_pred_scaled),\n",
        "               add_addn_reg_line=False)\n",
        "print('\\nOur predicted values fit the data well!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-otP7n7v4QJ",
        "colab_type": "text"
      },
      "source": [
        "## What we have learned\n",
        "\n",
        "\n",
        "\n",
        "*   In Machine Learning we often spend considerable effort getting data in a form that our models can learn from the data.\n",
        "* A single neuron model can learn the appropriate slope and offset for the data \n",
        "*   Building this model was made easier by TensorFlow's implementation of the Keras library\n",
        "*   But to learn from the data we need to pass the data through the model enough times to get the loss to converge\n",
        "*   When we do that, the model learns the data's slope and offset more accurately.\n",
        "*  And when we apply these learned values, the predict prices from the test data's Square Footage are reasonable. \n",
        "\n"
      ]
    }
  ]
}